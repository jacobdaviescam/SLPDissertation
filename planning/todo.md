# To-Do List

- [x] Finish writing the conversion script
    - [x] Questions with dangling 'to'
    - [x] Deal with passives
        - [x] Lemma of 'was' is 'be'
    - [x] Deal with unaccusatives (these are linked with passives and the way that we deal with them)
        - [x] Dictionary mapping first
    - [x] Deal with 'that'
    - [x] Deal with 'to'
    - [x] Deal with center embeddings
    - [x] Add code to convert each sentence in the dataset
    - [x] Add code to include the distribution and text of the sentence
    - [x] Punctuation
        - [x] Identify the main verb (the one that has no incoming edges)
    - [x] Unittests
    - [x] Sanity Checks
        - [x] Check that there is only one root
        - [x] Check that the length of forms, lemmas, pos, heads, deprel are the same
- [ ] Alter the parser code
    - [ ] Add BERT embeddings
    - [x] Add a way to evaluate with respect to the distribution
        - [x] Line 179 in conll17_ud_eval.py - do we need to change this?
- [x] Write introduction chapter
- [ ] Write background chapter
    - [x] Contextual Embeddings
    - [x] Feature Representations
    - [ ] Entangled Representations
- [x] Conduct experiments for data collection
    - [x] Run experiments on Cirrus
        - [x] Start by running a parse of a subset of the dataset
        - [x] Write a slurm script
        - [x] Run the parser on the entire dataset
    - [x] Rerun the training on both the transition and graph based parsers - as there is an error with the training script. 
    - [x] Rerun the conversion on Q_long_mv as the punctuation is wrong in the gold.
        - [x] Find a way to get this type back into the generalisation dataset. 
    - [ ] Run the transition model with less and more stack elements
    - [ ] Run the graph-based model and force projectivity
    - [ ] Run both graph and transition one more time and then average the result over the different seeds - report seeds for each random initialisation. 
- [ ] Analyze collected data
    - [ ] Labeled attachment score by sentence length 
    - [ ] Labeled F-score by dependency length
    - [ ] Labeled F-score by distance to root
    - [ ] Labeled precision and recall for non-projective dependencies
    - [ ] Accuracies on different parts of speech
    - [ ] What is the effect of using transition vs graph based parsers on different types of syntactic structures?
    - [ ] What is the effect of using external BERT embeddings on the performance of the parser?
- [x] Write methodology chapter
    - [x] How did I create the dataset
- [x] Revise literature review
- [ ] Write results and discussion chapter
    - [ ] What analysis techniques do I want to do?
    - [ ] t-SNE plot 
- [ ] Write conclusion chapter
- [ ] Proofread and edit entire dissertation
- [ ] Submit final dissertation
